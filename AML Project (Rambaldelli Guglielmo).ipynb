{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST\n",
    "Rambaldelli Guglielmo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Required preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#install libray required\n",
    "import os\n",
    "print(os.system('sudo apt-get install python-numpy python-scipy python-sklearn python-pip python-matplotlib'))\n",
    "print(os.system('pip3 install mlxtend tensorflow keras'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create directory and download database\n",
    "import os\n",
    "os.chdir('/tmp')\n",
    "os.system('mkdir AML')\n",
    "%cd 'AML'\n",
    "os.system('curl -o  train-images-idx3-ubyte.gz http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz')\n",
    "os.system('curl -o train-labels-idx1-ubyte.gz http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz')\n",
    "os.system('curl -o t10k-images-idx3-ubyte.gz http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz')\n",
    "os.system('curl -o t10k-labels-idx1-ubyte.gz http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz')\n",
    "os.system('gunzip train-images-idx3-ubyte.gz')\n",
    "os.system('gunzip train-labels-idx1-ubyte.gz')\n",
    "os.system('gunzip t10k-images-idx3-ubyte.gz')\n",
    "os.system('gunzip t10k-labels-idx1-ubyte.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import required \n",
    "from mlxtend.data import loadlocal_mnist\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression                         \n",
    "from sklearn.tree import DecisionTreeClassifier                             \n",
    "from sklearn.neighbors import KNeighborsClassifier                          \n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis        \n",
    "from sklearn.naive_bayes import GaussianNB                                  \n",
    "from sklearn.svm import SVC         \n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.svm import OneClassSVM\n",
    "\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.metrics import classification_report  \n",
    "from sklearn.metrics import confusion_matrix \n",
    "\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Load test and train set\n",
    "\n",
    "X_train, Y_train = loadlocal_mnist(\n",
    "        images_path='train-images-idx3-ubyte', \n",
    "        labels_path='train-labels-idx1-ubyte')\n",
    "\n",
    "X_test, Y_test = loadlocal_mnist(\n",
    "        images_path='t10k-images-idx3-ubyte', \n",
    "        labels_path='t10k-labels-idx1-ubyte')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First af all we tried to compare the various algorithm we studied on this specific problem to see wich can be suitablie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "models.append(( 'LR'   , LogisticRegression()))\n",
    "models.append(( 'LDA'  , LinearDiscriminantAnalysis()))\n",
    "models.append(( 'KNN'  , KNeighborsClassifier()))\n",
    "models.append(( 'CART' , DecisionTreeClassifier()))\n",
    "models.append(( 'NB'   , GaussianNB()))\n",
    "models.append(( 'SVM'  , SVC()))\n",
    "models.append(( 'MLP'  , MLPClassifier()))\n",
    "models.append(( 'RF'  , RandomForestClassifier()))\n",
    "\n",
    "\n",
    "results = []\n",
    "names = []\n",
    "scoring = 'accuracy'\n",
    "for name, model in models:\n",
    "  model.fit(X_train, Y_train)\n",
    "  cv_results =  model.score(X_test, Y_test) \n",
    "  results.append(cv_results)\n",
    "  names.append(name)\n",
    "  msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "  print(msg)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the literature we saw that a succesfull aproac used was SVM so we experimanted with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Support Vector\n",
    "models = []\n",
    "models.append(( 'linear svc'   ,sklearn.svm.LinearSVC()))\n",
    "models.append(( 'Nu'  jupyter notebook,sklearn.svm.NuSVC()))\n",
    "models.append(( 'Svc'  ,sklearn.svm.SVC()))\n",
    "\n",
    "\n",
    "results = []\n",
    "names = []\n",
    "scoring = 'accuracy'\n",
    "for name, model in models:\n",
    "    model.fit(X_train, Y_train)\n",
    "    cv_results =  model.score(X_test, Y_test)\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "We saw that SVC had the best perforance so we tried a grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C=[0.1,0.5,1,2,5,10]\n",
    "D=[3,4,5,6,7,8,9,10]\n",
    "seed=[1234,5678,7890,3456,2345,6789]\n",
    "\n",
    "with open(\"SVMcomp.txt\", \"w\") as SVMcomp:\n",
    "    for s in S:\n",
    "        for c in C:\n",
    "                for d in D:\n",
    "                        model = sklearn.svm.SVC(C=c,degree=d,random_state=s  )  \n",
    "                        # Evaluate using Cross Validation\n",
    "                        num_folds = 5\n",
    "                        seed = 1234\n",
    "                        kfold = KFold(n_splits=num_folds, random_state=seed)\n",
    "                        results = cross_val_score(model, X_train, Y_train, cv=kfold)\n",
    "                        SVMcomp.write(x,s,results.mean()*100.0, results.std()*100.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the best one seems to be KNN, SVM, RF and MLP we keept experimenting on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest\n",
    "s=[1,10,100]\n",
    "for s in size:\n",
    "    model = RandomForestClassifier(n_estimators=100, n_jobs=-1, )        \n",
    "    model.fit(X_train, Y_train)             \n",
    "    print(s,model.score(X_test,Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KNN\n",
    "s=[1,10,100]\n",
    "for s in size:\n",
    "    model = KNeighborsClassifier(leaf_size=10,n_neighbors=10, weights='distance',n_jobs=-1 ) \n",
    "    model.fit(X_train, Y_train)             \n",
    "    print(s,model.score(X_test,Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunatly with KNN, RF and SVM we could not reach the same accuracy that MLP could."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Voting Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We tried to complement them using a Voting System both with 'hard' and 'soft' voting but the imbalance in the methods used gave us worst result then single method as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Voting CLassifier\n",
    "LR=LogisticRegression()\n",
    "LDA=LinearDiscriminantAnalysis()\n",
    "KNN=KNeighborsClassifier()\n",
    "CART=DecisionTreeClassifier()\n",
    "NB=GaussianNB()\n",
    "SVC=SVC()\n",
    "MLP=MLPClassifier()\n",
    "RF=RandomForestClassifier()\n",
    "\n",
    "model = VotingClassifier(estimators=[\n",
    "        ('LR', LR), ('LDA', LDA), ('KNN', KNN), ('CART', CART), ('NB', NB), ('SVC', SVC), ('MLP', MLP), ('RF', RF)], voting='hard')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate using Cross Validation\n",
    "num_folds = 2\n",
    "seed = 1234\n",
    "kfold = KFold(n_splits=num_folds, random_state=seed)\n",
    "results = cross_val_score(model, X_train, Y_train, cv=kfold)\n",
    "print(\"Accuracy: %.3f%% (%.3f%%)\" % (results.mean()*100.0, results.std()*100.0))\n",
    "\n",
    "\n",
    "#test\n",
    "model = model.fit(X_train, Y_train)\n",
    "print(model.score('test',X_test,Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-layer Perceptron classifier "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After some testing we saw that MPL was the best performing method so decided to try optimize MLP Classifier.\n",
    "(Warning: really long execution time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MLP comparison\n",
    "\n",
    "\n",
    "layers=[(500,300,100),(300,500,100),(100,300,500),(100,500,100),(100,500,300),(500,1000,500),(100,500,1000,100),(100,1000,500,100),(50,1000,100),(728,1000,100),(728,100,500,100),(728,100,1000,50),(728,364,1500,100),(728,10,100,10),(728,500,100)]\n",
    "\n",
    "seed=['1234','2345','3456','4567','5678','6789','7890','8901','9012']\n",
    "\n",
    "for x in layers:\n",
    "        for s in seed:\n",
    "            with open('MLPcomp.txt','w') as out:\n",
    "                    g=int(s)\n",
    "                    print(s,g)\n",
    "                    model = MLPClassifier(hidden_layer_sizes=x, max_iter=1000,learning_rate_init=0.001,random_state=g)\n",
    "                    # Evaluate using Cross Validation\n",
    "                    num_folds = 2\n",
    "                    seedk = 1234\n",
    "                    kfold = KFold(n_splits=num_folds, random_state=seedk)\n",
    "                    results = cross_val_score(model, X_train, Y_train, cv=kfold)\n",
    "                    print(x, s, results.mean()*100.0, results.std()*100.0)\n",
    "                    out.write(x+' '+s+' '+results.mean()*100.0+' '+results.std()*100.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select good (over 97 %) one and phrase them\n",
    "print(os.popen('cat MLPcomp.txt | awk '$NF > 97 {print}' > clean.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#best setup\n",
    "layers=[(500,300,100),(300,500,100),(100,300,500),(100,500,100),(100,500,300),(500,1000,500),(100,500,1000,100),(100,1000,500,100),(50,1000,100),(728,1000,100),(728,100,500,100),(728,100,1000,50),(728,364,1500,100),(728,10,100,10),(728,500,100)]\n",
    "\n",
    "\n",
    "(728, 364, 728, 364) 1234 97.00666666666666 0.12666666666666937\n",
    "(728, 364, 728, 364) 3456 97.09 0.4500000000000004\n",
    "(728, 364, 728, 364) 5678 97.22666666666666 0.0033333333333385173\n",
    "(728, 50, 75, 100, 50) 1234 97.1 0.009999999999998899\n",
    "(728, 50, 75, 100, 50) 3456 97.22500000000001 0.10833333333333806\n",
    "(728, 50, 75, 100, 50) 4567 97.255 0.10166666666666657\n",
    "(728, 50, 75, 100, 50) 7890 97.12833333333333 0.22166666666667\n",
    "(728, 50, 75, 100, 50) 8901 97.12 0.06666666666666488\n",
    "(728, 500, 300, 200, 100) 1234 97.11333333333334 0.1233333333333364\n",
    "(728, 500, 300, 200, 100) 7890 97.37 0.13333333333332975\n",
    "(728, 300, 500, 200, 100) 4567 97.01166666666667 0.07833333333333026\n",
    "(728, 300, 500, 200, 100) 7890 97.30833333333334 0.24500000000000077\n",
    "(728, 300, 500, 200, 100) 9012 97.12833333333333 0.11166666666667102\n",
    "(728, 100, 200, 300, 100) 2345 97.315 0.22833333333333594\n",
    "(728, 100, 200, 300, 100) 3456 97.30666666666667 0.02666666666666928\n",
    "(728, 100, 200, 300, 100) 4567 97.06666666666666 0.1433333333333342\n",
    "(728, 100, 200, 300, 100) 6789 97.125 0.3416666666666679\n",
    "(728, 364, 728, 364) 1234 97.00666666666666 0.12666666666666937\n",
    "(728, 364, 728, 364) 3456 97.09 0.4500000000000004\n",
    "(728, 364, 728, 364) 5678 97.22666666666666 0.0033333333333385173"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup voting classifiers using good performing layout\n",
    "with open('clean.txt','r') as best:\n",
    "    MLP=[]\n",
    "    bestL= best.readlines()\n",
    "    name=0\n",
    "    for m in bestL:\n",
    "        l=m.split(' ')\n",
    "        seed=int(l[-3])\n",
    "        layer=eval(' '.join(l[0:-3]))\n",
    "        mlp=MLPClassifier(hidden_layer_sizes=layer, max_iter=1000,learning_rate_init=0.001,random_state=seed)\n",
    "        MLP.append((str(name),mlp))\n",
    "        name=name+1\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Voting CLassifier with MLP (hard)\n",
    "model = VotingClassifier(estimators=MLP,  voting='hard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Voting CLassifier with MLP (soft)\n",
    "model = VotingClassifier(estimators=MLP,  voting='soft')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test\n",
    "model = model.fit(X_train, Y_train)\n",
    "print(model.score('test',X_test,Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#metrics\n",
    "predicted = model.predict(X_test)\n",
    "report = classification_report(Y_test, predicted) #report\n",
    "\n",
    "matrix = confusion_matrix(Y_test, predicted) #confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using both hard and soft voting we managed to reach an accuracy of 98.8 %\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional NN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We saw from litterature that the best way to aproach this kind of problems are convolutional neural network,\n",
    "given that sklearn dose not support them we decided to use keras and tensorflow as a backhand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import to_categorical\n",
    "from keras import optimizers\n",
    "\n",
    "#download mnist data and split into train and test sets\n",
    "X_train, Y_train = loadlocal_mnist(\n",
    "        images_path='train-images-idx3-ubyte', \n",
    "        labels_path='train-labels-idx1-ubyte')\n",
    "\n",
    "X_test, Y_test = loadlocal_mnist(\n",
    "        images_path='t10k-images-idx3-ubyte', \n",
    "        labels_path='t10k-labels-idx1-ubyte')\n",
    "\n",
    "#reshape data to fit model\n",
    "X_train = X_train.reshape(60000,28,28,1)\n",
    "X_test = X_test.reshape(10000,28,28,1)\n",
    "#one-hot encode target column\n",
    "Y_train = to_categorical(Y_train)\n",
    "Y_test = to_categorical(Y_test)\n",
    "#create model\n",
    "model = Sequential()\n",
    "#add model layers\n",
    "model.add(Conv2D(32, (3, 3), activation='sigmoid'))\n",
    "model.add(Conv2D(32, (3, 3), activation='sigmoid'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Dropout(0.05))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(100, activation='sigmoid'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "#compile model using accuracy to measure model performance\n",
    "opt = optimizers.SGD(lr=0.01, momentum=0.9)\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    " \n",
    "#train the model\n",
    "model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using convolutional network we managed to reach an accuracy of 99.2%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
